from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from setfit import SetFitModel
import torch
import time

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±ÛŒ
device = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_PATH = "model/persian_intent_model"

app = FastAPI(title="Persian Intent Detection Service")

# Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ (Singleton Pattern)
# Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù…Ø¯Ù„ ÙÙ‚Ø· ÛŒÚ©Ø¨Ø§Ø± Ø¯Ø± VRAM Ù„ÙˆØ¯ Ø´ÙˆØ¯
try:
    print(f"ğŸš€ Loading model to {device}...")
    model = SetFitModel.from_pretrained(MODEL_PATH).to(device)
    print("âœ… Model is ready for inference.")
except Exception as e:
    print(f"âŒ Error loading model: {e}")
    model = None

class Query(BaseModel):
    text: str

@app.get("/health")
def health_check():
    return {"status": "up", "device": device, "model_loaded": model is not None}

@app.post("/predict")
async def predict(data: Query):
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        start_time = time.perf_counter()
        
        # Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø±ÙˆÛŒ GPU Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú¯Ø±Ø§Ø¯ÛŒÙ†Øª Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±
        with torch.no_grad():
            intent = model(data.text)
            
        end_time = time.perf_counter()
        latency = (end_time - start_time) * 1000
        
        return {
            "intent": str(intent),
            "latency_ms": round(latency, 2),
            "device": torch.cuda.get_device_name(0) if device == "cuda" else "cpu"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8025)